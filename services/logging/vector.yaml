# Ingests GELF logs from Docker daemon and forwards to Loki and Graylog (both over TCP)

sources:
  # Receive GELF messages from Docker containers via UDP
  vector_metrics:
    type: internal_metrics
    scrape_interval_secs: ${PROMETHEUS_SCRAPE_INTERVAL_SECONDS}
  docker_gelf:
    type: socket
    address: "0.0.0.0:12201"
    mode: udp
    decoding:
      codec: gelf
    framing:
      method: chunked_gelf
      # Auto-detect compression (gzip, zlib, or uncompressed)
      decompression: Auto

transforms:
  # Process and enrich the logs
  process_logs_base:
    type: remap
    inputs: ["docker_gelf"]
    source: |
      # Make sure to retain .host from the GELF message (originating host), do not overwrite
      # Map short_message to message for Loki compatibility
      if exists(.short_message) {
        .message = .short_message
      }

      # Handle container name - GELF uses _container_name (with underscore prefix)
      if exists(._container_name) {
        .container_name = ._container_name

        # Extract Docker service name from container name (everything before first dot)
        match = parse_regex!(.container_name, r'^(?P<service_name>[^.]+)')
        .service_name = match.service_name
      } else {
        .container_name = "unknown"
        .service_name = "unknown"
      }
      .log_service = .service_name

      # Handle container ID
      if exists(._container_id) {
        .container_id = ._container_id
      } else {
        .container_id = "unknown"
      }

      # Remove the confusing Docker creation timestamp
      if exists(._created) {
        del(._created)
      }
      # Also handle the variant without underscore if it exists
      if exists(.created) {
        del(.created)
      }

      # Handle image name
      if exists(._image_name) {
        .image_name = ._image_name
      } else {
        .image_name = "unknown"
      }

      # Add processing metadata
      .processed_by = "vector"

      # Extract structured log fields from message using regex pattern
      if exists(.message) {
        # python service logs
        parsed_fields, err = parse_regex(.message, r'log_level=(?P<log_level>[^|]*) \| log_timestamp=(?P<log_timestamp>[^|]*) \| log_source=(?P<log_source>[^|]*) \| log_uid=(?P<log_uid>[^|]*) \| log_oec=(?P<log_oec>[^|]*) \| log_trace_id=(?P<log_trace_id>[^|]*) \| log_span_id=(?P<log_span_id>[^|]*) \| log_msg=(?P<log_msg>.*)$')
        # traefik logs
        traefik_fields, traefik_err = parse_regex(.message, r'time="(?P<log_timestamp>[^"]*)" level=(?P<log_level>\w+) msg="(?P<log_msg>.*)"')
        if err == null {
          .log_level = parsed_fields.log_level
          .log_timestamp = parsed_fields.log_timestamp
          .log_source = parsed_fields.log_source
          .log_uid = parsed_fields.log_uid
          .log_oec = parsed_fields.log_oec
          .log_trace_id = parsed_fields.log_trace_id
          .log_span_id = parsed_fields.log_span_id
          .log_msg = parsed_fields.log_msg
        } else if traefik_err == null {
          .log_timestamp = traefik_fields.log_timestamp
          .log_level = traefik_fields.log_level
          .log_msg = traefik_fields.log_msg
        } else {
          .log_msg = .message
        }

        log_msg_processed = replace!(.log_msg, "\\n", "
        ")
        if log_msg_processed != null {
          .log_msg = log_msg_processed
        }
      }

  process_logs_loki:
    type: "remap"
    inputs: ["process_logs_base"]
    source: |
      # ensure level field is set for Grafana Loki
      if exists(.log_level){
        .level = .log_level
      } else {
        .level = "UNKNOWN"
      }

sinks:
  # Send to Loki over TCP
  loki:
    type: loki
    inputs: ["process_logs_loki"]
    endpoint: "http://${VECTOR_LOG_DESTINATION:?err}:12204"
    encoding:
      codec: json
    labels:
      source: "vector"
      host: "{{ host }}"
      container_name: "{{ container_name }}"
      service_name: "{{ service_name }}"

    healthcheck:
      enabled: true
  prometheus_exporter: # https://vector.dev/docs/administration/monitoring/#metrics
    type: prometheus_exporter
    inputs:
      - vector_metrics
    address: "0.0.0.0:9598"
  # Send to Graylog via GELF over TCP
  graylog:
    type: socket
    inputs: ["process_logs_base"]
    address: "${VECTOR_LOG_DESTINATION:?err}:12203"
    mode: tcp
    encoding:
      codec: gelf
    healthcheck:
      enabled: true

  # https://docs.victoriametrics.com/victorialogs/data-ingestion/vector/
  kubernetes-victoria-logs:
    inputs:
      - process_logs_base
    # https://vector.dev/docs/reference/configuration/sinks/elasticsearch/
    type: elasticsearch
    auth:
      strategy: basic
      user: "${SERVICES_USER}"
      password: "${SERVICES_PASSWORD}"
    endpoints:
    - https://${K8S_MONITORING_FQDN}/logs/insert/elasticsearch
    api_version: v8
    compression: gzip
    healthcheck:
      enabled: false
    mode: bulk
    request:
      # https://docs.victoriametrics.com/victorialogs/data-ingestion/#http-headers
      headers:
        # https://docs.victoriametrics.com/victorialogs/#multitenancy
        AccountID: "0"
        ProjectID: "0"
        # first non-empty field from the list is treated as log message
        # WARNING: the message field with value is being renamed to `_msg`
        # (original field key is dropped / not present in VictoriaLogs GUI)
        VL-Msg-Field: log_msg,message
        # log field names, which uniquely identify every log stream
        VL-Stream-Fields: service_name,container_name,container_id
        VL-Time-Field: timestamp
        # WARNING: we overwrite source_type from `socket` to `docker_swarm_logs`
        # This makes more sense to users of victoria logs. Users can use filters
        # source_type="kubernetes_logs" (set automatically for kubernetes logs)
        # or source_type="docker_swarm_logs"
        VL-Extra-Fields: source_type=docker_swarm_logs
        # ignore internal fields to save space (e.g. _command, _image_id, etc.)
        VL-Ignore-Fields: _*
    tls:
      verify_certificate: false
      verify_hostname: false

  # Temporary: Output to console for debugging
  #console_debug:
  #  type: console
  #  inputs: ["process_logs"]
  #  encoding:
  #    codec: json

# Global configuration
api:
  enabled: true
  address: "0.0.0.0:8686"

data_dir: "/var/lib/vector"
