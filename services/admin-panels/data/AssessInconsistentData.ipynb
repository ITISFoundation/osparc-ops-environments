{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Administration: Assess Inconsistant Data from oSparc database and S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone oSparc Repo and install python dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI: Output is supressed here for better readability. To debug, remove the '%%capture' at the end of the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "!git clone https://github.com/ITISFoundation/osparc-simcore.git\n",
    "!python3 -m pip install requests\n",
    "!python3 -m pip install sqlalchemy\n",
    "!python3 -m pip install psycopg2-binary\n",
    "!python3 -m pip install boto3\n",
    "!python3 -m pip install pandas\n",
    "!python3 -m pip install tqdm\n",
    "!python3 -m pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!cd osparc-simcore/packages/postgres-database && pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config: Endpoints and Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "osparcURL = os.environ.get('MACHINE_FQDN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceendpointurl = os.environ.get('S3_ENDPOINT')\n",
    "sourcebucketname = os.environ.get('S3_BUCKET')\n",
    "sourcebucketaccess = os.environ.get('S3_ACCESS_KEY')\n",
    "sourcebucketsecret= os.environ.get('S3_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin: Main script\n",
    "### Import python mocules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Python\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import importlib\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "# S3\n",
    "import warnings\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "# pgSQL\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import psycopg2\n",
    "# Pandas and Widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "# tqdm progressbar\n",
    "from tqdm.notebook import tqdm\n",
    "# Osparc-Simcore\n",
    "import simcore_postgres_database\n",
    "from simcore_postgres_database.models.projects import projects\n",
    "from simcore_postgres_database.models.users import users\n",
    "from simcore_postgres_database.models.file_meta_data import file_meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect S3\n",
    "warnings.filterwarnings('ignore', '.*Adding certificate verification is strongly advised.*', )\n",
    "############ Common functionality\n",
    "def isObjectPresentOnBucket(botobucket, filepath,allObjectsBucket = None,filelist=None):\n",
    "    if filelist != None:\n",
    "        return filepath in filelist\n",
    "    else:\n",
    "        if allObjectsBucket == None:\n",
    "            objs = list(botobucket.objects.filter(Prefix=filepath))\n",
    "        else:\n",
    "            objs = list(allObjectsBucket.filter(Prefix=filepath))\n",
    "        if len(objs) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "# Configure source bucket\n",
    "# via https://docs.min.io/docs/how-to-use-aws-sdk-for-python-with-minio-server.html\n",
    "src_s3 = boto3.resource('s3',\n",
    "                    endpoint_url=sourceendpointurl,\n",
    "                    aws_access_key_id=sourcebucketaccess,\n",
    "                    aws_secret_access_key=sourcebucketsecret,\n",
    "                    config=Config(signature_version='s3v4'),\n",
    "                    region_name='us-east-1',\n",
    "                    verify=False)\n",
    "src_bucket = src_s3.Bucket(sourcebucketname)\n",
    "filelist = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch all files from S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files from S3, create filelist\n",
    "print(\"WARNING: THIS MAKE TAKE A WHILE DEPENDING ON BUCKET SIZE\")\n",
    "print(\"Running...\")\n",
    "allObjectsSourceBucket = src_bucket.objects.all()\n",
    "filelist = set({i.key for i in allObjectsSourceBucket})\n",
    "# Total list of files on S3:\n",
    "filesOnS3 = filelist\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note bene: If you plan on mutating JSON fields, see this [https://amercader.net/blog/beware-of-json-fields-in-sqlalchemy ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to pgSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PG_PASSWORD = os.environ.get('POSTGRES_PASSWORD')\n",
    "PG_PUBLIC_ENDPOINT=os.environ.get('MACHINE_FQDN')\n",
    "PG_DB=os.environ.get('POSTGRES_DB')\n",
    "PG_PORT=os.environ.get('POSTGRES_PORT')\n",
    "PG_USER=os.environ.get('POSTGRES_USER')\n",
    "pgEngineURL= \"postgresql://{user}:{password}@{host}:{port}/{database}\".format(\n",
    "        user=PG_USER,\n",
    "        password=PG_PASSWORD,\n",
    "        database=PG_DB,\n",
    "        host=PG_PUBLIC_ENDPOINT,\n",
    "        port=int(PG_PORT),\n",
    "    )\n",
    "engine = db.create_engine(pgEngineURL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "metadata = db.MetaData()\n",
    "####\n",
    "# Get database tables as pandas df objects\n",
    "users_df = pd.read_sql_table(\n",
    "    'users',\n",
    "    con=engine\n",
    ")\n",
    "projects_df = pd.read_sql_table(\n",
    "    'projects',\n",
    "    con=engine\n",
    ")\n",
    "files_meta_data_df = pd.read_sql_table(\n",
    "    'file_meta_data',\n",
    "    con=engine\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate projects table, find files referenced in project/workbench/nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundFilePathsInProjectsTable_WorkbenchInputs = []\n",
    "foundInstances_WorkbenchInput = []\n",
    "foundFilePathsInProjectsTable_WorkbenchOutputs = []\n",
    "foundInstances_WorkbenchOutput = []\n",
    "# I .Iterate projects table, iterate workbench of every project, find inputs/outputs that are of key 'path', whitelist them\n",
    "for i,instance in enumerate(session.query(projects).order_by(projects.c.id)):\n",
    "    listOfNodesInProject = instance.workbench.keys()\n",
    "    for key in listOfNodesInProject:\n",
    "        inputsOutputsPresent = False\n",
    "        if 'inputs' in instance.workbench[key].keys() and instance.workbench[key]['inputs'] != {}:\n",
    "            #print(\"Inputs: \",instance.workbench[key]['inputs'])\n",
    "            inputsOutputsPresent = True\n",
    "            for j in instance.workbench[key]['inputs'].keys():\n",
    "                # Ducktype check if input obj is dict:\n",
    "                try:\n",
    "                    items = instance.workbench[key]['inputs'][j].items()\n",
    "                except (AttributeError, TypeError):\n",
    "                    continue\n",
    "                else: # is dict\n",
    "                    for k in instance.workbench[key]['inputs'][j].keys():\n",
    "                        if k == 'path':\n",
    "                            foundFilePathsInProjectsTable_WorkbenchInputs.append(instance.workbench[key]['inputs'][j][k])\n",
    "                            foundInstances_WorkbenchInput.append(instance)\n",
    "        if 'outputs' in instance.workbench[key].keys() and instance.workbench[key]['outputs'] != {}:\n",
    "            #print(\"Outputs: \",instance.workbench[key]['outputs'])\n",
    "            for j in instance.workbench[key]['outputs'].keys():\n",
    "                # Ducktype check if input obj is dict:\n",
    "                try:\n",
    "                    items = instance.workbench[key]['outputs'][j].items()\n",
    "                except (AttributeError, TypeError):\n",
    "                    continue\n",
    "                else: # is dict\n",
    "                    for k in instance.workbench[key]['outputs'][j].keys():\n",
    "                        if k == 'path':\n",
    "                            foundFilePathsInProjectsTable_WorkbenchOutputs.append(instance.workbench[key]['outputs'][j][k])\n",
    "                            foundInstances_WorkbenchOutput.append(instance)\n",
    "            inputsOutputsPresent = True\n",
    "# Find files we expect to be present, but they are not\n",
    "garbageFilesInProjectsWorkbench = set(foundFilePathsInProjectsTable_WorkbenchInputs + foundFilePathsInProjectsTable_WorkbenchOutputs).difference( set(filesOnS3))\n",
    "print(\"Number of files MISSING according to projects/workbench references: \",len(garbageFilesInProjectsWorkbench))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate file_meta_data table, resolve file location S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "statement = db.select([file_meta_data])\n",
    "ResultProxy = session.execute(statement)\n",
    "flag = True\n",
    "file_meta_data_files = []\n",
    "file_meta_data_files_NOT_ON_S3 = []\n",
    "file_meta_data_files_filesizeMinusOne_ON_S3 = []\n",
    "file_meta_data_files_filesizeMinusOne_NOT_ON_S3 = []\n",
    "countFoundByUuid = 0\n",
    "countFoundByObjectName = 0\n",
    "countFoundByFileID = 0\n",
    "countFoundByRawPath = 0\n",
    "debugList = []\n",
    "print(\"Running analysis for file_meta_data table, this can take a while...\")\n",
    "# We paginate, and fetch the table in a while loop piece-by-piece\n",
    "# For every file, we check if it is actually present on the S3 bucket, and if the file_size attribute == -1.\n",
    "### Healthy files: Present on S3 and filesize != -1\n",
    "### Healable files: Present on S3 and filesize == -1\n",
    "### Damaged files: Not present on S3\n",
    "while flag:\n",
    "    partial_results = ResultProxy.fetchmany(50)\n",
    "    if partial_results == []:\n",
    "        flag = False\n",
    "        ResultProxy.close()\n",
    "    else:\n",
    "        for i,instance in enumerate(partial_results):\n",
    "            if instance.is_soft_link == 1:\n",
    "                if isObjectPresentOnBucket(src_bucket,instance.file_id,filelist=filelist):\n",
    "                    file_meta_data_files.append(str(instance.file_id))\n",
    "                else:\n",
    "                    file_meta_data_files_NOT_ON_S3.append(str(instance.file_id))\n",
    "            elif instance.file_size == -1:\n",
    "                \n",
    "                if isObjectPresentOnBucket(src_bucket,instance.object_name,filelist=filelist):\n",
    "                    file_meta_data_files_filesizeMinusOne_ON_S3.append(str(instance.object_name))\n",
    "                else:\n",
    "                    file_meta_data_files_filesizeMinusOne_NOT_ON_S3.append(str(instance.object_name))\n",
    "            else:\n",
    "                if isObjectPresentOnBucket(src_bucket,instance.object_name,filelist=filelist):\n",
    "                    file_meta_data_files.append(str(instance.object_name))\n",
    "                else:\n",
    "                    file_meta_data_files_NOT_ON_S3.append(str(instance.object_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabled: Recover S3 files from versioning whereever possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deletedFilesRecoverableOnS3UsingVersioning = []\n",
    "if False:\n",
    "    for i in tqdm(file_meta_data_files_NOT_ON_S3 + file_meta_data_files_filesizeMinusOne_NOT_ON_S3):\n",
    "        versions = src_bucket.object_versions.filter(Prefix=i)\n",
    "        for version in versions:\n",
    "            curVersion = \"null\"\n",
    "            try:\n",
    "                curVersion = version.get().get('VersionId')\n",
    "            except:\n",
    "                pass\n",
    "            if curVersion != \"null\":\n",
    "                deletedFilesRecoverableOnS3UsingVersioning.append(i)\n",
    "                break                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")\n",
    "print(\"Preliminary Summary:\")\n",
    "print(\"##########\")\n",
    "# File Size -1: File never finished uploading -> Can be removed after checking that they dont exist in S3\n",
    "print(\"Number of files present on S3 but noted as corrupt in pgSQL (filesize == -1, healable): \", len(file_meta_data_files_filesizeMinusOne_ON_S3))\n",
    "print(\"Number of files not present on S3 and noted as corrupt in pgSQL (filesize == -1): \", len(file_meta_data_files_filesizeMinusOne_NOT_ON_S3))\n",
    "print(\"Number of MISSING files referenced in file_meta_data: \", len(file_meta_data_files_NOT_ON_S3))\n",
    "print(\"Number of MISSING files recoverable using bucket versioning: \", len(deletedFilesRecoverableOnS3UsingVersioning))\n",
    "print(\"##########\")\n",
    "print(\"Number of files healthy: \", len(file_meta_data_files))\n",
    "print(\"Number of files damaged: \", len(file_meta_data_files_NOT_ON_S3 + file_meta_data_files_filesizeMinusOne_NOT_ON_S3))\n",
    "print(\"Number of files healable: \", len(file_meta_data_files_filesizeMinusOne_ON_S3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dig deeper into the properties of missing files & inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know these files to be valid, and store the full path to the file in the whitelist\n",
    "whitelistedFiles = list(set(file_meta_data_files + foundFilePathsInProjectsTable_WorkbenchInputs + foundFilePathsInProjectsTable_WorkbenchOutputs))\n",
    "# Find files present on S3 but not referenced in the DB\n",
    "garbageFilesOnS3 = set(filesOnS3 - set(whitelistedFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running additional analysis for file_meta_data table, this can take a while...\")\n",
    "# Get total list of files that are damaged (we expect them to be present, they are not):\n",
    "garbageFilesInFileMetaData = garbageFilesInProjectsWorkbench.union(set(file_meta_data_files_NOT_ON_S3)).union(set(file_meta_data_files_filesizeMinusOne_NOT_ON_S3))\n",
    "#\n",
    "# we built so-called boolean-lists, which is a way to filter pandas dataframes.\n",
    "# Get list of projects that contain damaged files\n",
    "maskProjectsContainingFilesMissingOnS3 = projects_df.uuid.isin([i.split('/')[0] for i in garbageFilesInFileMetaData])\n",
    "#############################\n",
    "#\n",
    "#\n",
    "# To be safe, copy, the dataframe\n",
    "files_meta_data_df_copy = copy.deepcopy(files_meta_data_df)\n",
    "\n",
    "### MASKS\n",
    "maskAssocProjectNotInProjectDB = ~files_meta_data_df_copy.project_id.isin(projects_df['uuid'])\n",
    "maskAssocProjectValid = ~maskAssocProjectNotInProjectDB\n",
    "maskProjectValidAndNodeNonExistant = files_meta_data_df_copy['object_name'].apply(lambda x: sum(projects_df.uuid.isin([x.split(\"/\")[0]])) == 1 \\\n",
    "                                                                                                and not x.split(\"/\")[1] in json.dumps(projects_df[projects_df.uuid.isin([x.split(\"/\")[0]])].iloc[0]['workbench']))\n",
    "maskProjectNonExistantOrNodeNonExistant = files_meta_data_df_copy['object_name'].apply(lambda x: sum(projects_df.uuid.isin([x.split(\"/\")[0]])) == 0 or (sum(projects_df.uuid.isin([x.split(\"/\")[0]])) == 1 \\\n",
    "                                                                                                and not x.split(\"/\")[1] in json.dumps(projects_df[projects_df.uuid.isin([x.split(\"/\")[0]])].iloc[0]['workbench'])))\n",
    "maskDoesntContainLog = files_meta_data_df_copy['object_name'].apply(lambda x: \"log\" not in x)\n",
    "maskDoesntContainZip = files_meta_data_df_copy['object_name'].apply(lambda x: \"zip\" not in x)\n",
    "# These files are API Files with project_id == NULL, dont delete them\n",
    "maskTheseAreAPIFiles = ~files_meta_data_df_copy.apply(lambda x: str(x.project_id) in str(x.object_name), axis=1) | files_meta_data_df_copy['file_id'].apply(lambda x: \"api/\" in x) | files_meta_data_df_copy.is_soft_link.isin([\"1\"])\n",
    "maskObjectNotReferencedInProjectsWorkbench = ~files_meta_data_df_copy.object_name.isin(foundFilePathsInProjectsTable_WorkbenchInputs + foundFilePathsInProjectsTable_WorkbenchOutputs)\n",
    "maskObjectReferencedInProjectWorkbench = ~maskObjectNotReferencedInProjectsWorkbench\n",
    "maskFileIsMissingOnS3 = files_meta_data_df_copy.object_name.isin(garbageFilesInFileMetaData)\n",
    "#\n",
    "# After a talk with Sylvain, we filter a bit more: \n",
    "# A file present on file_meta_data, present on S3, with a valid projectID, not referenced in projects/workbench is NOT an indication of a broken project\n",
    "# if: The file is not a zip file AND the filename does not contain \"log\"\n",
    "# else: The entry in file_meta_data can be deleted\n",
    "maskCanBeDeleted1 = maskAssocProjectValid & maskObjectNotReferencedInProjectsWorkbench\n",
    "maskCanBeDeleted1 = maskCanBeDeleted1 & maskDoesntContainLog\n",
    "maskCanBeDeleted1 = maskCanBeDeleted1 & maskDoesntContainZip\n",
    "maskCanBeDeleted1 = maskCanBeDeleted1 & ~maskTheseAreAPIFiles\n",
    "#\n",
    "#\n",
    "maskCanBeDeleted2 = maskProjectValidAndNodeNonExistant & ~maskFileIsMissingOnS3\n",
    "maskCanBeDeleted2 = maskCanBeDeleted2 & maskObjectNotReferencedInProjectsWorkbench\n",
    "#\n",
    "maskCanBeDeleted3 = maskProjectNonExistantOrNodeNonExistant & ~maskTheseAreAPIFiles \n",
    "maskCanBeDeleted3 = maskCanBeDeleted3 & maskObjectNotReferencedInProjectsWorkbench\n",
    "#\n",
    "#\n",
    "#\n",
    "maskFilesExpectedValid = ~maskCanBeDeleted1 & ~maskCanBeDeleted2 & ~maskCanBeDeleted3\n",
    "maskCanBeDeleted = ~maskFilesExpectedValid\n",
    "#\n",
    "maskFilesInFileMetaData_ExpectedValidButActuallyBroken = maskFilesExpectedValid & maskFileIsMissingOnS3\n",
    "#\n",
    "#\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"########\")\n",
    "print(\"Files / db-entries to be cleaned-up:\")\n",
    "print(\"Number of files in S3 not referenced in the database: \", len(garbageFilesOnS3))\n",
    "print(\"    --> These S3 files should be deleted.\")\n",
    "print(\"Number of file_meta_data entries that are not API files and not referenced in projects/workbench, where assoc. projectID is non-existant: \", sum(maskAssocProjectNotInProjectDB & ~maskTheseAreAPIFiles & maskObjectNotReferencedInProjectsWorkbench))\n",
    "print(\"Number of file_meta_data entries that are not API files and not referenced in projects/workbench, with a valid projectID, and filename doesn't contain log or zip: \",sum(maskCanBeDeleted1))\n",
    "print(\"Number of file_meta_data entries that are not API files and not referenced in projects/workbench, with a valid projectID, but the nodeID is non-existant: \",sum(maskCanBeDeleted2))\n",
    "print(\"    --> These DB entries and associated S3 files should be deleted\")\n",
    "print(\"Number of files missing in S3 but referenced in file_meta_data entries: \",sum(maskFileIsMissingOnS3))\n",
    "print(\"    ... from these files, number of files that we expected valid: \", sum(maskFilesInFileMetaData_ExpectedValidButActuallyBroken))\n",
    "print(\"    --> These DB entries should be deleted, and we should be aware of projects that still point to these files.\")\n",
    "print(\"TOTAL NUMBER of file_meta_data entries to be deleted: \", sum(maskCanBeDeleted) )\n",
    "print(\"########\")\n",
    "print(\"Broken / missing / inconsistent files or projects:\")\n",
    "print(\"Number of files referenced in projects/workbench not present in S3: \", len(garbageFilesInProjectsWorkbench))\n",
    "print(\"    --> The assoc. projects might not work anymore and the user should be informed.\")\n",
    "print(\"Number of healable files, where files are present on S3 but file_size in file_meta_data is -1 : \", len(file_meta_data_files_filesizeMinusOne_ON_S3))\n",
    "print(\"    --> These DB entries should be updated with the proper filesize.\")\n",
    "print(\"Number of projects with associated files in the file_meta_data table that are missing in S3: \",sum(maskProjectsContainingFilesMissingOnS3))\n",
    "print(\"    --> These projects *might* not work anymore and the user should be informed.\")\n",
    "print(\"Number of invalid file_meta_data entries referenced by projects: \",sum(maskFilesInFileMetaData_ExpectedValidButActuallyBroken))\n",
    "print(\"    --> These projects *will* not work anymore and the user should be informed.\")\n",
    "####\n",
    "if \"ASSESS_INCONSISTENT_DATA_CMD_RUN\" in os.environ:\n",
    "    returncode = len(garbageFilesOnS3)\n",
    "    returncode += sum(maskAssocProjectNotInProjectDB & ~maskTheseAreAPIFiles & maskObjectNotReferencedInProjectsWorkbench)\n",
    "    returncode += sum(maskCanBeDeleted1)\n",
    "    returncode += sum(maskCanBeDeleted2)\n",
    "    returncode += sum(maskFileIsMissingOnS3)\n",
    "    returncode += sum(maskFilesInFileMetaData_ExpectedValidButActuallyBroken)\n",
    "    returncode += sum(maskCanBeDeleted)\n",
    "    returncode += len(garbageFilesInProjectsWorkbench)\n",
    "    returncode += len(file_meta_data_files_filesizeMinusOne_ON_S3)\n",
    "    returncode += sum(maskProjectsContainingFilesMissingOnS3)\n",
    "    returncode += sum(maskFilesInFileMetaData_ExpectedValidButActuallyBroken)\n",
    "    if returncode != 0:\n",
    "        print(\"DATA INCONSISTENCIES DETECTED\")\n",
    "        exit(1) # This is invalid ipython syntax, which will cause an error, which in turn causes the ipython command to fail with exit code 1. :)\n",
    "exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUI and Analysis: Which user has broken projects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE HELPER FOR DROPDOWN\n",
    "# via https://towardsdatascience.com/bring-your-jupyter-notebook-to-life-with-interactive-widgets-bc12e03f0916\n",
    "ALL = 'ALL'\n",
    "def unique_sorted_values_plus_ALL(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    unique.insert(0, ALL)\n",
    "    return unique\n",
    "# Generate a list of users for the dropdown\n",
    "usersList = session.query(users).order_by(users.c.email)\n",
    "\n",
    "\n",
    "\n",
    "# DamagedProjects contains projects with files referenced in projects/workbench that are not there\n",
    "# Collect projects entries where damaged files are present\n",
    "damagedProjects = []\n",
    "for i,curItem in enumerate(garbageFilesInProjectsWorkbench):\n",
    "    if curItem in foundFilePathsInProjectsTable_WorkbenchInputs:\n",
    "        curIndex = foundFilePathsInProjectsTable_WorkbenchInputs.index(curItem)\n",
    "        projEntry = foundInstances_WorkbenchInput[curIndex]\n",
    "    else:\n",
    "        curIndex = foundFilePathsInProjectsTable_WorkbenchOutputs.index(curItem)\n",
    "        projEntry = foundInstances_WorkbenchOutput[curIndex]\n",
    "    damagedProjects.append(projEntry)\n",
    "\n",
    "\n",
    "damagedProjectsUUIDs = [i.uuid for i in damagedProjects]\n",
    "\n",
    "file_meta_data_df_broken_and_referenced = files_meta_data_df_copy[maskFilesInFileMetaData_ExpectedValidButActuallyBroken]\n",
    "# Add projects with broken file_meta_data entries\n",
    "boolean_series = projects_df.uuid.isin(damagedProjectsUUIDs)\n",
    "boolean_series = boolean_series | projects_df.uuid.isin(file_meta_data_df_broken_and_referenced.project_id)\n",
    "# Add projects with links to broken files in workbench\n",
    "for i in file_meta_data_df_broken_and_referenced.project_id.tolist():\n",
    "    boolean_series = boolean_series | projects_df.workbench.str.contains(str(i), case=False)\n",
    "# Add projects with associated broken file_meta_data files as matched by the projectUUID in the S3-filename\n",
    "for i in file_meta_data_df_broken_and_referenced.object_name.tolist():\n",
    "    boolean_series = boolean_series | projects_df.uuid.str.contains(str(i).split('/')[0], case=False)\n",
    "\n",
    "# Pandas Dataframe: Containing Broken projects\n",
    "df_garbage_in_database = projects_df[boolean_series]\n",
    "\n",
    "# Built dropdown...\n",
    "usersListFilter = users_df.id.isin(df_garbage_in_database.prj_owner)\n",
    "users_df = users_df[usersListFilter]\n",
    "dropdown_user = widgets.Dropdown(options  = unique_sorted_values_plus_ALL(users_df.email))\n",
    "print(\"Browse projects with missing files here, by project owner:\")\n",
    "display(dropdown_user)\n",
    "output_user = widgets.Output()\n",
    "display(output_user)\n",
    "ListOfCurrentSelectedProjectUUIDs = []\n",
    "def dropdown_user_eventhandler(change):\n",
    "    output_user.clear_output()\n",
    "    if (change.new == ALL):\n",
    "        with output_user:\n",
    "            display(df_garbage_in_database)\n",
    "    else:\n",
    "        with output_user:\n",
    "            # Now we need to go the reverse way, and for a given projectID find the broken files.\n",
    "            # Probably I should have gone for a hashtable right from the start, but oh well, here we are...\n",
    "            userID = users_df.loc[users_df['email'] == change.new].iloc[0].id\n",
    "            is_user_project = df_garbage_in_database['prj_owner']==userID\n",
    "            #booleanList = file_meta_data_df_broken_and_referenced.project_id.isin(df_garbage_in_database[is_user_project].uuid.tolist())\n",
    "            #for i in df_garbage_in_database[is_user_project].uuid.tolist():\n",
    "            #    booleanList = booleanList | file_meta_data_df_broken_and_referenced.object_name.str.contains(str(i), case=False)\n",
    "            display(df_garbage_in_database[is_user_project].sort_values(by=\"last_change_date\",ascending=False))\n",
    "            \n",
    "            for index, row in df_garbage_in_database[is_user_project].iterrows():\n",
    "                linkedFileList = []\n",
    "                workbenchItems = df_garbage_in_database[is_user_project].at[index,'workbench']\n",
    "                # For each project, get list of files referenced in project/workbench\n",
    "                for key in workbenchItems.keys():\n",
    "                    inputsOutputsPresent = False\n",
    "                    if 'inputs' in workbenchItems[key].keys() and workbenchItems[key]['inputs'] != {}:\n",
    "                        inputsOutputsPresent = True\n",
    "                        for j in workbenchItems[key]['inputs'].keys():\n",
    "                            # Ducktype check if input obj is dict:\n",
    "                            try:\n",
    "                                items = workbenchItems[key]['inputs'][j].items()\n",
    "                            except (AttributeError, TypeError):\n",
    "                                continue\n",
    "                            else: # is dict\n",
    "                                for k in workbenchItems[key]['inputs'][j].keys():\n",
    "                                    if k == 'path':\n",
    "                                        linkedFileList.append(workbenchItems[key]['inputs'][j][k])\n",
    "                    if 'outputs' in workbenchItems[key].keys() and workbenchItems[key]['outputs'] != {}:\n",
    "                        for j in workbenchItems[key]['outputs'].keys():\n",
    "                            # Ducktype check if input obj is dict:\n",
    "                            try:\n",
    "                                items = workbenchItems[key]['outputs'][j].items()\n",
    "                            except (AttributeError, TypeError):\n",
    "                                continue\n",
    "                            else: # is dict\n",
    "                                for k in workbenchItems[key]['outputs'][j].keys():\n",
    "                                    if k == 'path':\n",
    "                                        linkedFileList.append(workbenchItems[key]['outputs'][j][k])\n",
    "                        inputsOutputsPresent = True\n",
    "                uuid = df_garbage_in_database[is_user_project].at[index,'uuid']\n",
    "                # Get Broken entries in file_meta_data for this project\n",
    "                fileMetaDataGarbage = file_meta_data_df_broken_and_referenced.project_id.isin([uuid])\n",
    "                if sum(fileMetaDataGarbage) > 0:\n",
    "                    print(\"Broken (missing on S3) files referenced in file_meta_data for project \",uuid,\" :\\n\",file_meta_data_df_broken_and_referenced[fileMetaDataGarbage].object_name.tolist())\n",
    "                strings_with_substring = [string for string in garbageFilesInProjectsWorkbench if uuid in string]\n",
    "                for i in linkedFileList:\n",
    "                    strings_with_substring += [string for string in garbageFilesInProjectsWorkbench if i in string]\n",
    "                strings_with_substring = list(set(strings_with_substring))\n",
    "                if len(strings_with_substring) > 0:\n",
    "                    print(\"Broken (missing on S3) files referenced in projects/workbench for project \",uuid,\" :\\n\",strings_with_substring)\n",
    "\n",
    "                \n",
    "dropdown_user.observe(dropdown_user_eventhandler, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "--------------\n",
    "# Delete S3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDataPriorToDeletion = True\n",
    "print(\"Will delete \",len(garbageFilesOnS3), \" files on S3 unreferenced in osparc (not referenced in file_meta_data or projects/workbench).\")\n",
    "if input(\"Type YES to backup data to another bucket before deletion: \") == \"YES\":\n",
    "    print(\"Will back up data prior to deletion.\")\n",
    "    saveDataPriorToDeletion = True\n",
    "else:\n",
    "    saveDataPriorToDeletion = False\n",
    "    print(\"Will NOT back up data prior to deletion.\")\n",
    "if saveDataPriorToDeletion:\n",
    "    dest_s3 = boto3.resource('s3',\n",
    "                        endpoint_url='',\n",
    "                        aws_access_key_id='',\n",
    "                        aws_secret_access_key='',\n",
    "                        #config=Config(signature_version='s3v4'),\n",
    "                        region_name='us-east-1',\n",
    "                        verify=False)\n",
    "    saveBucketName = 'backupmaster' + datetime.today().strftime('%Y%m%d')\n",
    "    # Check if bucket already exists, if not, create\n",
    "    from botocore.client import ClientError\n",
    "    import time\n",
    "    try:\n",
    "        dest_s3.meta.client.head_bucket(Bucket=saveBucketName)\n",
    "    except ClientError:\n",
    "        dest_s3.create_bucket(Bucket=saveBucketName)\n",
    "    #\n",
    "    time.sleep(0.25)\n",
    "    dest_bucket = dest_s3.Bucket(saveBucketName)\n",
    "    \n",
    "def deleteDataListS3(inputlist,src_bucket,saveDataBucket = None, saveBucketName = None):\n",
    "    if input(\"enter YES again to start deletion\") == \"YES\":\n",
    "        i=0\n",
    "        pageIncrement = 500\n",
    "        while i + pageIncrement < len(inputlist):\n",
    "            print(\"Processing batch of \",pageIncrement,\" files...\")\n",
    "            deletionList = []\n",
    "            for j in range(i, i + pageIncrement):\n",
    "                deletionList.append(list(inputlist)[j])\n",
    "            if saveDataBucket != None:\n",
    "                for j in deletionList:\n",
    "                    copy_source = {\n",
    "                        'Bucket': str(saveBucketName),\n",
    "                        'Key': j\n",
    "                    }\n",
    "                    saveDataBucket.copy(copy_source, j)\n",
    "            time.sleep(0.10)\n",
    "            deletionDict = {'Objects':[{'Key': i} for i in deletionList]}\n",
    "            src_bucket.delete_objects(Delete=deletionDict)\n",
    "            i += pageIncrement\n",
    "        if i != len(inputlist):\n",
    "            print(\"Processing batch of \",len(inputlist) - i,\" files...\")\n",
    "            deletionList = []\n",
    "            for j in range(i, len(inputlist)):\n",
    "                deletionList.append(list(inputlist)[j])\n",
    "            if saveDataBucket != None:\n",
    "                for j in deletionList:\n",
    "                    copy_source = {\n",
    "                        'Bucket': str(saveBucketName),\n",
    "                        'Key': j\n",
    "                    }\n",
    "                    saveDataBucket.copy(copy_source, j)\n",
    "            time.sleep(0.50)\n",
    "            deletionDict = {'Objects':[{'Key': i} for i in deletionList]}\n",
    "            src_bucket.delete_objects(Delete=deletionDict)\n",
    "if input(\"enter YES to start deletion\") == \"YES\":\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y_%m_%d\")\n",
    "    filenameout = date_time + '_S3Deletion.txt'\n",
    "    with open(filenameout, 'a') as f:\n",
    "        for item in garbageFilesOnS3:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "        f.write(\"----------\\n\")\n",
    "    if saveDataPriorToDeletion:\n",
    "        deleteDataListS3(garbageFilesOnS3,src_bucket,dest_bucket,saveBucketName)\n",
    "    else:\n",
    "        deleteDataListS3(garbageFilesOnS3,src_bucket)\n",
    "else:\n",
    "    print(\"DELETION ABORTED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete file_meta_data entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markedForDeletionInFileMetaData = files_meta_data_df_copy[maskCanBeDeleted].object_name.to_list()\n",
    "print(\"Getting ready to delete \",len(markedForDeletionInFileMetaData), \" items in file_meta_data table.\")\n",
    "if input(\"enter YES to delete unused file references in file_meta_data table.\") == \"YES\":\n",
    "    with engine.connect() as connection:\n",
    "        file_meta_data_sqla = file_meta_data\n",
    "        process1 = set(markedForDeletionInFileMetaData).intersection(set(file_meta_data_files).union(file_meta_data_files_filesizeMinusOne_ON_S3))\n",
    "        process2 = set(markedForDeletionInFileMetaData).intersection(set(file_meta_data_files_NOT_ON_S3).union(file_meta_data_files_filesizeMinusOne_NOT_ON_S3))\n",
    "        #\n",
    "        statement = db.delete(file_meta_data_sqla).where(file_meta_data_sqla.c.object_name.in_(list(process2))).execution_options(synchronize_session=\"fetch\")\n",
    "        print(\"Executing 1/2\")\n",
    "        result = connection.execute(statement)\n",
    "        statement = db.delete(file_meta_data_sqla).where(file_meta_data_sqla.c.object_name.in_(list(process1))).execution_options(synchronize_session=\"fetch\")\n",
    "        print(\"Executing 2/2\")\n",
    "        result = connection.execute(statement)\n",
    "else:\n",
    "    print(\"DELETION ABORTED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "32335f2c9ba2d563ee666049f89be892e53f4981ed6f0e6ef6d361cd25d5ad71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
