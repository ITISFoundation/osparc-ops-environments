version: "3.8"
services:
  clusters-keeper:
    environment:
      - LOG_LEVEL=${LOG_LEVEL:-WARNING}
      - LOG_FORMAT_LOCAL_DEV_ENABLED=${LOG_FORMAT_LOCAL_DEV_ENABLED}
      - CLUSTERS_KEEPER_LOGLEVEL=${LOG_LEVEL:-INFO}
      - CLUSTERS_KEEPER_MAX_MISSED_HEARTBEATS_BEFORE_CLUSTER_TERMINATION=${CLUSTERS_KEEPER_MAX_MISSED_HEARTBEATS_BEFORE_CLUSTER_TERMINATION}
      - CLUSTERS_KEEPER_TASK_INTERVAL=${CLUSTERS_KEEPER_TASK_INTERVAL}
      - CLUSTERS_KEEPER_EC2_ACCESS_KEY_ID=${CLUSTERS_KEEPER_EC2_ACCESS_KEY_ID}
      - CLUSTERS_KEEPER_EC2_ENDPOINT=${CLUSTERS_KEEPER_EC2_ENDPOINT}
      - CLUSTERS_KEEPER_EC2_INSTANCES_ALLOWED_TYPES=${CLUSTERS_KEEPER_EC2_INSTANCES_ALLOWED_TYPES}
      - CLUSTERS_KEEPER_EC2_INSTANCES_AMI_ID=${CLUSTERS_KEEPER_EC2_INSTANCES_AMI_ID}
      - CLUSTERS_KEEPER_EC2_INSTANCES_KEY_NAME=${CLUSTERS_KEEPER_EC2_INSTANCES_KEY_NAME}
      - CLUSTERS_KEEPER_EC2_INSTANCES_SECURITY_GROUP_IDS=${CLUSTERS_KEEPER_EC2_INSTANCES_SECURITY_GROUP_IDS}
      - CLUSTERS_KEEPER_EC2_INSTANCES_SUBNET_ID=${CLUSTERS_KEEPER_EC2_INSTANCES_SUBNET_ID}
      - CLUSTERS_KEEPER_EC2_REGION_NAME=${CLUSTERS_KEEPER_EC2_REGION_NAME}
      - CLUSTERS_KEEPER_EC2_SECRET_ACCESS_KEY=${CLUSTERS_KEEPER_EC2_SECRET_ACCESS_KEY}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - RABBIT_HOST=${RABBIT_HOST}
      - RABBIT_PASSWORD=${RABBIT_PASSWORD}
      - RABBIT_PORT=${RABBIT_PORT}
      - RABBIT_SECURE=${RABBIT_SECURE}
      - RABBIT_USER=${RABBIT_USER}
    deploy:
      labels: []
      replicas: 0
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          memory: 200M # Determined from prometheus cadvisor actual usage on master, which was <100M
          cpus: '0.5' # For now: same as autoscaling service
        reservations:
          memory: 200M # Determined from prometheus cadvisor actual usage on master, which was <100M
          cpus: '0.5' # For now: same as autoscaling service
      placement:
        constraints:
          - node.labels.simcore==true

  autoscaling:
    environment:
      - EC2_ACCESS_KEY_ID=${AUTOSCALING_EC2_ACCESS_KEY_ID} # defines the access key to EC2
      - EC2_SECRET_ACCESS_KEY=${AUTOSCALING_EC2_SECRET_ACCESS_KEY} # defines the secret key to EC2
      - EC2_REGION_NAME=${AUTOSCALING_EC2_REGION_NAME}
      - EC2_INSTANCES_ALLOWED_TYPES=${AUTOSCALING_EC2_INSTANCES_ALLOWED_TYPES} # defines the allowed EC2 instance types (["t2.nano", ...])
      - EC2_INSTANCES_AMI_ID=${AUTOSCALING_EC2_INSTANCES_AMI_ID} # defines the AMI ID to use
      - EC2_INSTANCES_CUSTOM_BOOT_SCRIPTS=${AUTOSCALING_EC2_INSTANCES_CUSTOM_BOOT_SCRIPTS}
      - EC2_INSTANCES_MACHINES_BUFFER=${AUTOSCALING_EC2_INSTANCES_MACHINES_BUFFER} # defines the number of reserved machines
      - EC2_INSTANCES_MAX_INSTANCES=${AUTOSCALING_EC2_INSTANCES_MAX_INSTANCES} # maximum number of instances
      - EC2_INSTANCES_PRE_PULL_IMAGES=${AUTOSCALING_EC2_INSTANCES_PRE_PULL_IMAGES}
      - EC2_INSTANCES_SECURITY_GROUP_IDS=${AUTOSCALING_EC2_INSTANCES_SECURITY_GROUP_IDS} # defines the security groups for starting EC2s
      - EC2_INSTANCES_SUBNET_ID=${AUTOSCALING_EC2_INSTANCES_SUBNET_ID} # defines the subnet ID to use for the EC2 instances
      - EC2_INSTANCES_KEY_NAME=${AUTOSCALING_EC2_INSTANCES_KEY_NAME} # defines the used Key name for the EC2 instances
      - NODES_MONITORING_NODE_LABELS=${AUTOSCALING_NODES_MONITORING_NODE_LABELS} # defines the labels to check for on the nodes (i.e. ["dynamicsidecar"])
      - NODES_MONITORING_SERVICE_LABELS=${AUTOSCALING_NODES_MONITORING_SERVICE_LABELS} # optional defines the label to check for on the monitored services
      - NODES_MONITORING_NEW_NODES_LABELS=${NODES_MONITORING_NEW_NODES_LABELS} # optional defines the name of the monitored service
      - RABBIT_HOST=${RABBIT_HOST}
      - RABBIT_PASSWORD=${RABBIT_PASSWORD}
      - RABBIT_PORT=${RABBIT_PORT}
      - RABBIT_SECURE=${RABBIT_SECURE}
      - RABBIT_USER=${RABBIT_USER}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - REGISTRY_USER=${REGISTRY_USER}
      - REGISTRY_PW=${REGISTRY_PW}
      - REGISTRY_URL=${REGISTRY_URL}
      - REGISTRY_SSL=${REGISTRY_SSL}
      - REGISTRY_AUTH=${REGISTRY_AUTH}
    deploy:
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        reservations:
          cpus: "0.5"
          memory: "256M"
        limits:
          cpus: "0.5"
          memory: "512M"

  agent:
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - LOGLEVEL=WARNING
      - AGENT_VOLUMES_CLEANUP_TARGET_SWARM_STACK_NAME=${AGENT_VOLUMES_CLEANUP_TARGET_SWARM_STACK_NAME}
    deploy:
      update_config:
        parallelism: 2
        order: stop-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.dynamicsidecar == true
    extra_hosts: []
  api-server:
    networks:
      - public
      - monitored
    environment:
      - LOGSPOUT_MULTILINE=true
    deploy:
      labels:
        # NOTE: apiserver does not need sslheader since there is no socket.io
        - traefik.http.routers.${SWARM_STACK_NAME}_api-server.middlewares=${SWARM_STACK_NAME}_gzip@docker
        # internal traefik
        - io.simcore.zone=${TRAEFIK_SIMCORE_ZONE}
        - traefik.http.routers.${SWARM_STACK_NAME}_apiserver_swagger.service=${SWARM_STACK_NAME}_api-server
        - traefik.http.routers.${SWARM_STACK_NAME}_apiserver_swagger.rule=hostregexp(`{host:.+}`) && PathPrefix(`/dev/`)
        - traefik.http.routers.${SWARM_STACK_NAME}_apiserver_swagger.entrypoints=simcore_api
        - traefik.http.routers.${SWARM_STACK_NAME}_apiserver_swagger.priority=2
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.simcore==true
    extra_hosts: []

  catalog:
    environment:
      - LOGSPOUT_MULTILINE=true
    networks:
      - monitored
    deploy:
      replicas: 10
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.simcore==true

  static-webserver:
    environment:
      - LOGSPOUT_MULTILINE=true
    deploy:
      labels:
        # Handle freshping service and route it to the faster static webserver.
        - traefik.http.middlewares.${SWARM_STACK_NAME}_static_webserver_prefix.addprefix.prefix=/osparc
        - traefik.http.routers.${SWARM_STACK_NAME}_static_webserver_freshping.rule=HeadersRegexp(`User-Agent`, `.*(FreshpingBot).*`)
        - traefik.http.routers.${SWARM_STACK_NAME}_static_webserver_freshping.service=${SWARM_STACK_NAME}_static_webserver_freshping
        - traefik.http.services.${SWARM_STACK_NAME}_static_webserver_freshping.loadbalancer.server.port=8000
        - traefik.http.routers.${SWARM_STACK_NAME}_static_webserver_freshping.entrypoints=http
        - traefik.http.routers.${SWARM_STACK_NAME}_static_webserver_freshping.priority=10 # High number means high priority
        - traefik.http.routers.${SWARM_STACK_NAME}_static_webserver_freshping.middlewares=${SWARM_STACK_NAME}_gzip@docker,${SWARM_STACK_NAME}_static_webserver_retry,${SWARM_STACK_NAME}_static_webserver_prefix
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.simcore==true
  invitations:
    environment:
      - INVITATIONS_LOGLEVEL=${INVITATIONS_LOGLEVEL}
    deploy:
      labels:
        # internal traefik
        - traefik.enable=true
        - io.simcore.zone=${TRAEFIK_SIMCORE_ZONE}
        - traefik.http.routers.${SWARM_STACK_NAME}_invitations.rule=(${DEPLOYMENT_FQDNS_CAPTURE_INVITATIONS})
        - traefik.http.routers.${SWARM_STACK_NAME}_invitations.entrypoints=http
        - traefik.http.services.${SWARM_STACK_NAME}_invitations.loadbalancer.server.port=8000
        - traefik.http.routers.${SWARM_STACK_NAME}_invitations_swagger.rule=(${DEPLOYMENT_FQDNS_CAPTURE_INVITATIONS}) && PathPrefix(`/dev/doc`)
        - traefik.http.routers.${SWARM_STACK_NAME}_invitations_swagger.entrypoints=http
        - traefik.http.middlewares.${SWARM_STACK_NAME_NO_HYPHEN}_invitations_swagger_auth.basicauth.users=${TRAEFIK_USER}:${TRAEFIK_PASSWORD}
        - traefik.http.routers.${SWARM_STACK_NAME}_invitations_swagger.middlewares=${SWARM_STACK_NAME_NO_HYPHEN}_invitations_swagger_auth, ${SWARM_STACK_NAME_NO_HYPHEN}_sslheader
      placement:
        constraints:
          - node.labels.simcore==true
  webserver:
    networks:
      - public
      - monitored
    environment:
      - LOGSPOUT_MULTILINE=true
      - DIAGNOSTICS_MAX_AVG_LATENCY=10
      - WEBSERVER_STATIC_MODULE_STATIC_WEB_SERVER_URL=http://${PREFIX_STACK_NAME}_static-webserver:8000
      # temporaries for helping to debug issues with responsivity
      - PYTHONTRACEMALLOC=1
      - AIODEBUG_SLOW_DURATION_SECS=0.5
      - GUNICORN_CMD_ARGS=--timeout=90
      - PROJECTS_MAX_COPY_SIZE_BYTES=${PROJECTS_MAX_COPY_SIZE_BYTES}
      - WEBSERVER_LOGLEVEL=${WEBSERVER_LOGLEVEL}
      - WEBSERVER_ANNOUNCEMENTS=${WEBSERVER_ANNOUNCEMENTS}
    deploy:
      labels:
        # ssl header necessary so that socket.io upgrades correctly from polling to websocket mode. the middleware must be attached to the right connection.
        # NOTE: traefik does not like - in the sslheader middleware, so we override it here
        # NOTE: in deploy mode with SSL they must be set to https!
        - traefik.http.middlewares.${SWARM_STACK_NAME_NO_HYPHEN}_sslheader.headers.customrequestheaders.X-Forwarded-Proto=https
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver.middlewares=${SWARM_STACK_NAME}_gzip@docker, ${SWARM_STACK_NAME_NO_HYPHEN}_sslheader
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver_swagger.service=${SWARM_STACK_NAME}_webserver
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver_swagger.middlewares=${SWARM_STACK_NAME}_gzip@docker, ${SWARM_STACK_NAME_NO_HYPHEN}_sslheader
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver_swagger.rule=hostregexp(`{host:.+}`) && PathPrefix(`/dev/`)
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver_swagger.entrypoints=http
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver_swagger.priority=2
        - traefik.http.middlewares.${SWARM_STACK_NAME_NO_HYPHEN}_webserver_swagger_auth.basicauth.users=${TRAEFIK_USER}:${TRAEFIK_PASSWORD}
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver_swagger.middlewares=${SWARM_STACK_NAME_NO_HYPHEN}_webserver_swagger_auth, ${SWARM_STACK_NAME_NO_HYPHEN}_sslheader
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      replicas: 3
      placement:
        constraints:
          - node.labels.simcore==true

    extra_hosts: []

  wb-db-event-listener:
    hostname: "{{.Service.Name}}"
    environment:
      - WEBSERVER_LOGLEVEL=${WEBSERVER_LOGLEVEL}
    networks:
      - default
    deploy:
      # NOTE: https://github.com/ITISFoundation/osparc-simcore/pull/4286
      # NOTE: this MUSTN'T change, or weird things might happen
      # this will stay until all legacy dynamic services are gone.
      replicas: 1
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.simcore == true
  wb-garbage-collector:
    environment:
      - WEBSERVER_LOGLEVEL=${WEBSERVER_LOGLEVEL}
    networks:
      - default
      - interactive_services_subnet
    deploy:
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.role==manager
  storage:
    environment:
      - LOGSPOUT_MULTILINE=true
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - STORAGE_DEFAULT_PRESIGNED_LINK_EXPIRATION_SECONDS=${STORAGE_DEFAULT_PRESIGNED_LINK_EXPIRATION_SECONDS}
      - STORAGE_CLEANER_INTERVAL_S=${STORAGE_CLEANER_INTERVAL_S}
    networks:
      - monitored
    deploy:
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.simcore==true
  director:
    # Certificate necessary for local deploiement only
    #secrets:
    #- source: rootca.crt
    #target: /usr/local/share/ca-certificates/osparc.crt
    environment:
      - LOGSPOUT_MULTILINE=true
      - SIMCORE_SERVICES_NETWORK_NAME=${SWARM_STACK_NAME}_interactive_services_subnet
      - REGISTRY_SSL=${REGISTRY_SSL}
      # needed to pass the self-signed certificate to the spawned services
      #- DIRECTOR_SELF_SIGNED_SSL_FILENAME=/usr/local/share/ca-certificates/osparc.crt
      # - DIRECTOR_SELF_SIGNED_SSL_SECRET_ID=some_id
      #- DIRECTOR_SELF_SIGNED_SSL_SECRET_NAME=rootca.crt
      #- SSL_CERT_FILE=/usr/local/share/ca-certificates/osparc.crt
    networks:
      - monitored
    deploy:
      replicas: 5
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
  director-v2:
    environment:
      - LOGSPOUT_MULTILINE=true
      - SIMCORE_SERVICES_NETWORK_NAME=${SWARM_STACK_NAME}_interactive_services_subnet
      - REGISTRY_SSL=${REGISTRY_SSL}
      - DIRECTOR_V2_DEV_FEATURES_ENABLED=${DIRECTOR_V2_DEV_FEATURES_ENABLED}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - S3_SECURE=${S3_SECURE}
      - R_CLONE_PROVIDER=${R_CLONE_PROVIDER}
      - DYNAMIC_SIDECAR_LOG_LEVEL=${DYNAMIC_SIDECAR_LOG_LEVEL}
      - DIRECTOR_V2_LOGLEVEL=${DIRECTOR_V2_LOGLEVEL}
      # needed to pass the self-signed certificate to the spawned services
      #- DIRECTOR_SELF_SIGNED_SSL_FILENAME=/usr/local/share/ca-certificates/osparc.crt
      # - DIRECTOR_SELF_SIGNED_SSL_SECRET_ID=some_id
      #- DIRECTOR_SELF_SIGNED_SSL_SECRET_NAME=rootca.crt
      #- SSL_CERT_FILE=/usr/local/share/ca-certificates/osparc.crt
    networks:
      - monitored
    deploy:
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
  dask-sidecar:
    networks:
      - monitored
    environment:
      - LOGSPOUT_MULTILINE=true
    deploy:
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.dasksidecar==true
  dask-scheduler:
    networks:
      - public
    environment:
      - LOGSPOUT_MULTILINE=true
    deploy:
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.simcore==true
      labels:
        - traefik.enable=true
        - traefik.docker.network=${PUBLIC_NETWORK}
        - traefik.http.services.${PREFIX_STACK_NAME}_dask_scheduler.loadbalancer.server.port=8787
        - traefik.http.routers.${PREFIX_STACK_NAME}_dask_scheduler.rule=Host(`${MONITORING_DOMAIN}`) && PathPrefix(`/${PREFIX_STACK_NAME}_dask`)
        - traefik.http.routers.${PREFIX_STACK_NAME}_dask_scheduler.entrypoints=https
        - traefik.http.routers.${PREFIX_STACK_NAME}_dask_scheduler.tls=true
        - traefik.http.middlewares.${PREFIX_STACK_NAME}_dask_scheduler_replace_regex.replacepathregex.regex=^/${PREFIX_STACK_NAME}_dask/(.*)$$
        - traefik.http.middlewares.${PREFIX_STACK_NAME}_dask_scheduler_replace_regex.replacepathregex.replacement=/$${1}
        - traefik.http.routers.${PREFIX_STACK_NAME}_dask_scheduler.middlewares=${PREFIX_STACK_NAME}_dask_scheduler_replace_regex@docker, ops_gzip@docker, ops_auth@docker
      resources:
        limits:
          memory: 256M
          cpus: '1.000'
        reservations:
          memory: 256M
          cpus: '1.000'
  datcore-adapter:
    environment:
      - LOGSPOUT_MULTILINE=true
    deploy:
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.simcore==true
  migration:
    environment:
      - LOGSPOUT_MULTILINE=true
    deploy:
      resources:
        limits:
          memory: 180M
          cpus: '1.000'
        reservations:
          memory: 180M
      placement:
        constraints:
          - node.labels.simcore==true
  rabbit:
    networks:
      - monitored
      - public
    environment:
      - LOGSPOUT_MULTILINE=true
    volumes:
      - rabbit_data:/var/lib/rabbitmq
    deploy:
      labels:
        - traefik.enable=true
        - traefik.docker.network=${PUBLIC_NETWORK}
        - traefik.http.services.${PREFIX_STACK_NAME}_rabbit.loadbalancer.server.port=15672
        - traefik.http.routers.${PREFIX_STACK_NAME}_rabbit.rule=Host(`${MONITORING_DOMAIN}`) && PathPrefix(`/${PREFIX_STACK_NAME}_rabbit`)
        - traefik.http.routers.${PREFIX_STACK_NAME}_rabbit.entrypoints=https
        - traefik.http.routers.${PREFIX_STACK_NAME}_rabbit.tls=true
        - traefik.http.middlewares.${PREFIX_STACK_NAME}_rabbit_replace_regex.replacepathregex.regex=^/${PREFIX_STACK_NAME}_rabbit/(.*)$$
        - traefik.http.middlewares.${PREFIX_STACK_NAME}_rabbit_replace_regex.replacepathregex.replacement=/$${1}
        - traefik.http.routers.${PREFIX_STACK_NAME}_rabbit.middlewares=${PREFIX_STACK_NAME}_rabbit_replace_regex@docker, ops_gzip@docker
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.rabbit==true
  redis:
    networks:
      - monitored
    environment:
      - LOGSPOUT_MULTILINE=true
    deploy:
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.redis==true
  resource-usage-tracker:
    networks:
      - monitored
      - public
    hostname: "{{.Service.Name}}"
    deploy:
      # NOTE: https://github.com/ITISFoundation/osparc-simcore/pull/4286
      # NOTE: this MUSTN'T change, or weird things might happen
      # this will stay until all legacy dynamic services are gone.
      replicas: 1
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.simcore==true

  postgres:
    networks:
      - monitored
      - public
    environment:
      - LOGSPOUT_MULTILINE=true
    # in clusters one or more nodes are typically defined as THE postgres nodes.
    #   deploy:
    #     placement:
    #       constraints:
    #         - node.labels.postgres==true
    deploy:
      labels:
        - traefik.enable=true
        - io.simcore.zone=${TRAEFIK_SIMCORE_ZONE}
        - traefik.docker.network=${PUBLIC_NETWORK}
        - traefik.tcp.services.${SWARM_STACK_NAME}_postgres.loadBalancer.server.port=5432
        - traefik.tcp.routers.postgres.service=${SWARM_STACK_NAME}_postgres
        - traefik.tcp.routers.postgres.entrypoints=postgres
        - traefik.tcp.routers.postgres.tls=false
        - traefik.tcp.routers.postgres.tls.certresolver=myresolver
        - traefik.tcp.routers.postgres.rule=HostSNI(`*`)
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.role==manager
  traefik:
    networks:
      - monitored
      - public
    deploy:
      labels:
        # Prometheus
        - prometheus-job=traefik_simcore
        - prometheus-port=8082
        # external traefik
        - traefik.enable=true
        - traefik.docker.network=${PUBLIC_NETWORK}
        # oSparc web
        - traefik.http.services.${SWARM_STACK_NAME}_simcore_http.loadbalancer.server.port=80
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_http.entrypoints=https
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_http.tls=true
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_http.middlewares=ops_gzip@docker, ops_sslheader@docker, ops_ratelimit@docker
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_http.service=${SWARM_STACK_NAME}_simcore_http
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_http.rule=((${DEPLOYMENT_FQDNS_CAPTURE_TRAEFIK_RULE_CATCHALL}) && PathPrefix(`/`)) || ( (PathPrefix(`/dashboard`) || PathPrefix(`/api`) ) && Host(`traefikdashboard.${MACHINE_FQDN}`))
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_http.priority=1 # Lowest possible priority, maintenance page takes priority "2" (higher, maintenance page has precedent) if it is up
        # oSparc publicAPI
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_api.rule=(${DEPLOYMENT_API_DOMAIN_CAPTURE_TRAEFIK_RULE}) && PathPrefix(`/`)
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_api.entrypoints=https
        - traefik.http.services.${SWARM_STACK_NAME}_simcore_api.loadbalancer.server.port=10081
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_api.tls=true
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_api.middlewares=ops_gzip@docker, ops_ratelimit@docker
        - traefik.http.routers.${SWARM_STACK_NAME}_simcore_api.service=${SWARM_STACK_NAME}_simcore_api
        # oSparc non rate limited webAPI for testing
        - traefik.http.services.${SWARM_STACK_NAME}_testing_simcore_http.loadbalancer.server.port=80
        - traefik.http.routers.${SWARM_STACK_NAME}_testing_simcore_http.service=${SWARM_STACK_NAME}_testing_simcore_http
        - traefik.http.routers.${SWARM_STACK_NAME}_testing_simcore_http.rule=(${DEPLOYMENT_FQDNS_TESTING_CAPTURE_TRAEFIK_RULE})
        - traefik.http.routers.${SWARM_STACK_NAME}_testing_simcore_http.entrypoints=https
        - traefik.http.routers.${SWARM_STACK_NAME}_testing_simcore_http.tls=true
        - traefik.http.routers.${SWARM_STACK_NAME}_testing_simcore_http.middlewares=ops_gzip@docker, ops_sslheader@docker, ops_auth@docker, ops_whitelist_ips@docker
        # oSparc non rate limited publicAPI for testing
        - traefik.http.services.${SWARM_STACK_NAME}_testing_simcore_api.loadbalancer.server.port=10081
        - traefik.http.routers.${SWARM_STACK_NAME}_testing_simcore_api.service=${SWARM_STACK_NAME}_testing_simcore_api
        - traefik.http.routers.${SWARM_STACK_NAME}_testing_simcore_api.rule=(${DEPLOYMENT_API_DOMAIN_TESTING_CAPTURE_TRAEFIK_RULE})
        - traefik.http.routers.${SWARM_STACK_NAME}_testing_simcore_api.entrypoints=https
        - traefik.http.routers.${SWARM_STACK_NAME}_testing_simcore_api.tls=true
        - traefik.http.routers.${SWARM_STACK_NAME}_testing_simcore_api.middlewares=ops_gzip@docker, ops_sslheader@docker, ops_auth@docker, ops_whitelist_ips@docker
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.traefik==true
  traefik_api:
    # NOTE: this is a trick to allow to access the internal traefik REST API
    # A comment
    # list router like so: curl https://domain/api/http/routers | jq
    image: busybox:1.35.0
    command: sleep 900000d
    networks:
      - default
    deploy:
      labels:
        # route to internal traefik
        - io.simcore.zone=${TRAEFIK_SIMCORE_ZONE}
        # traefik UI
        - traefik.enable=true
        - traefik.http.routers.${SWARM_STACK_NAME}_traefik_api.service=api@internal
        - traefik.http.routers.${SWARM_STACK_NAME}_traefik_api.rule=(PathPrefix(`/dashboard`) || PathPrefix(`/api`) ) && Host(`traefikdashboard.${MACHINE_FQDN}`)
        - traefik.http.routers.${SWARM_STACK_NAME}_traefik_api.entrypoints=http
        - traefik.http.routers.${SWARM_STACK_NAME}_traefik_api.priority=2
        - traefik.http.routers.${SWARM_STACK_NAME}_traefik_api.middlewares=${SWARM_STACK_NAME}_auth@docker, ${SWARM_STACK_NAME}_whitelist_ips@docker
        - traefik.http.services.${SWARM_STACK_NAME}_traefik_api.loadbalancer.server.port=8080
        # Middlewares
        # basic authentication
        - traefik.http.middlewares.${SWARM_STACK_NAME}_auth.basicauth.users=${TRAEFIK_USER}:${TRAEFIK_PASSWORD}
        # OPS IP Whitelist
        - traefik.http.middlewares.${SWARM_STACK_NAME}_whitelist_ips.ipwhitelist.sourcerange=${TRAEFIK_IPWHITELIST_SOURCERANGE}
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.traefik==true
  whoami:
    image: "containous/whoami:v1.5.0"
    networks:
      - default
    # NOTE: this service allows better understanding of how the host gets forwarded inside the simcore stack
    deploy:
      labels:
        # internal traefik
        - io.simcore.zone=${TRAEFIK_SIMCORE_ZONE}
        # basic authentication
        - traefik.http.middlewares.${SWARM_STACK_NAME}_auth.basicauth.users=${TRAEFIK_USER}:${TRAEFIK_PASSWORD}
        # whoami
        - traefik.enable=true
        - traefik.http.services.${SWARM_STACK_NAME}_whoami.loadbalancer.server.port=80
        - traefik.http.routers.${SWARM_STACK_NAME}_whoami.rule=hostregexp(`{host:.+}`) && PathPrefix(`/whoami`)
        - traefik.http.routers.${SWARM_STACK_NAME}_whoami.entrypoints=http
        - traefik.http.routers.${SWARM_STACK_NAME}_whoami.priority=2
        - traefik.http.routers.${SWARM_STACK_NAME}_whoami.middlewares=${SWARM_STACK_NAME}_auth@docker,${SWARM_STACK_NAME}_gzip@docker
      update_config:
        parallelism: 2
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.ops==true

  payments:
    deploy:
      placement:
        constraints:
          - node.labels.simcore==true
      labels:
        - traefik.enable=true
        - io.simcore.zone=${TRAEFIK_SIMCORE_ZONE}
        - traefik.http.routers.${SWARM_STACK_NAME}_payments_api.rule=(${DEPLOYMENT_FQDNS_CAPTURE_PAYMENTS}) && PathPrefix(`/v1`)
        - traefik.http.routers.${SWARM_STACK_NAME}_payments_api.entrypoints=http
        - traefik.http.services.${SWARM_STACK_NAME}_payments.loadbalancer.server.port=8000
        - traefik.http.routers.${SWARM_STACK_NAME}_payments.rule=(${DEPLOYMENT_FQDNS_CAPTURE_PAYMENTS})
        - traefik.http.routers.${SWARM_STACK_NAME}_payments.entrypoints=http
        - traefik.http.middlewares.${SWARM_STACK_NAME_NO_HYPHEN}_payments_auth.basicauth.users=${TRAEFIK_USER}:${TRAEFIK_PASSWORD}
        - traefik.http.routers.${SWARM_STACK_NAME}_payments.middlewares=${SWARM_STACK_NAME_NO_HYPHEN}_payments_auth, ${SWARM_STACK_NAME_NO_HYPHEN}_sslheader

volumes:
  rabbit_data:
    name: ${SWARM_STACK_NAME}_rabbit_data
networks:
  public:
    external: true
    name: ${OPS_PUBLIC_NETWORK}
  monitored:
    external: true
    name: ${OPS_MONITORED_NETWORK}
  storage_subnet:
    attachable: true
    name: ${SWARM_STACK_NAME}_storage_subnet
  interactive_services_subnet:
    name: ${SWARM_STACK_NAME}_interactive_services_subnet
    driver: overlay
    attachable: true
    internal: false
    labels:
      com.simcore.description: "interactive services network"
    ipam:
      driver: default
      config:
        # FIXME: move to base docker-compose.yml
        - subnet: ${SIMCORE_INTERACTIVE_NETWORK_SUBNET}
  computational_services_subnet:
    name: ${SWARM_STACK_NAME}_computational_services_subnet
    attachable: true
# self-signed only
secrets:
  # These two are needed for local=deployments
  rootca.crt:
    external: true
  storageca.crt:
    external: true
